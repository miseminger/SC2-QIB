{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fc99f928",
   "metadata": {},
   "source": [
    "The learning objectives for this notebook are:\n",
    "\n",
    "* work with compressed data mounted from `ongaeshi`\n",
    "* subset `peroba` alignment and metadata files (by region and date), excluding Wuhan-Hu-1\n",
    "* find the n nearest global neighbors to each of those, and append their sequences and metadata to my files\n",
    "* remove redundant neighbors and duplicated sequences\n",
    "* create a maximum likelihood tree with `IQTREE2`\n",
    "* run `TreeTime` to update the branch lengths based on sample dates\n",
    "* do a mugration model in `TreeTime` based on region\n",
    "* make a tree diagram in `R` to display the mugration model, based on the notebooks from Leo\n",
    "    \n",
    "It should be noted that this notebook won't run just as is (it could with some adjustments) but is just a record of the shell commands I ran on my VM, `madeline-01`.\n",
    "\n",
    "It has been scrambled since its original iteration, for streamlining."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0b83213",
   "metadata": {},
   "outputs": [],
   "source": [
    "ssh madeline-01\n",
    "mkdir 20220217 #directory for today's data\n",
    "cd ~/ongaeshi-mnt/databases/\n",
    "\n",
    "#metadata: peroba_meta.220209_173253.tsv.xz\n",
    "#alignment: peroba_align.220209_173253.aln.xz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b96f8187",
   "metadata": {},
   "source": [
    "Look at the metadata structure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7a08021",
   "metadata": {},
   "outputs": [],
   "source": [
    "#look at the metadata structure:\n",
    "xzcat peroba_meta.220209_173253.tsv.xz | head -10\n",
    "\n",
    "#columns are:\n",
    "#strain\tgisaid_id\tdate\tlocation\tlocation_gisaid\tlocation_coguk\tage\tgender\tgisaid_clade\tpango_lineage\ttimestamp\tfreq_ACGT\tfreq_N\tseq_hash\n",
    "#note that dates are of the form: 2021-12-09"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a83e9ce2",
   "metadata": {},
   "source": [
    "Create a metadata file and a `.txt` file of sequence names to use, for all the sequences from England, between July 1, 2021 to February 28, 2022."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0581baac",
   "metadata": {},
   "outputs": [],
   "source": [
    "#subset English sequences from July 1, 2021 to February 28, 2022 (which hasn't happened yet, but it'll catch everything)\n",
    "\n",
    "#save the header (column names) to a tsv\n",
    "xzcat peroba_meta.220209_173253.tsv.xz | head -1 > /home/ubuntu/20220217/peroba_meta.220209_173253.England.tsv\n",
    "\n",
    "#subset by 'England/NORW-' using xzgrep, and append this to the .tsv with header names you just made\n",
    "xzgrep 'England/' peroba_meta.220209_173253.tsv.xz >> /home/ubuntu/20220217/peroba_meta.220209_173253.England.tsv\n",
    "\n",
    "#subset by date: all rows between two dates\n",
    "python3\n",
    ">>> import pandas as pd\n",
    ">>> df = pd.read_csv('/home/ubuntu/20220217/peroba_meta.220209_173253.England.tsv', parse_dates=['date'], sep='\\t', header=0)\n",
    ">>> df['date'].min() #2020-09-15 is the earliest sample\n",
    ">>> date_subset = df.loc[df['date'].between('2021-07-01','2022-02-28', inclusive='both')]\n",
    "#this gives 1941 rows\n",
    "\n",
    "#save relevant metadata as .tsv\n",
    ">>> date_subset.to_csv('/home/ubuntu/20220217/peroba_meta.220209_173253.England.20210701_20220228.tsv', sep='\\t', header=True, index=False)\n",
    "#save record names (format: \"England/NORW-XXXXXXX/YYYY\" to include as a .txt\n",
    ">>> date_subset['strain'].to_csv('/home/ubuntu/20220217/peroba_meta.220209_173253.England.20210701_20220228.seqnames.txt', sep='\\t', header=False, index=False)\n",
    "\n",
    ">>> Ctrl+D  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4f37486",
   "metadata": {},
   "source": [
    "Now subset the big fasta file to match, and get rid of any duplicate records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "077409f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#subset the fasta file using the .txt file of record names: this step is a bit slow\n",
    "nohup goalign subset -f /home/ubuntu/20220217/peroba_meta.220209_173253.England.20210701_20220228.seqnames.txt -i peroba_align.220209_173253.aln.xz -o /home/ubuntu/20220217/peroba_align.220209_173253.England.20210701_20220228.aln\n",
    "#20220221: this takes ~25 minutes\n",
    "\n",
    "#I think another way to do this would be to do: xzgrep -w -A 1 -f  /home/ubuntu/20220217/peroba_meta.220209_173253.England.20210701_20220228.seqnames.txt peroba_align.220209_173253.aln.xz --no-group-separator\n",
    "#note that goalign subset converts the fasta records from two-line to multi-line; this might be what messes with uvaia later on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff916bf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "seqkit stats /home/ubuntu/20220217/peroba_align.220209_173253.England.20210701_20220228.aln\n",
    "\n",
    "file                                                                            format  type  num_seqs        sum_len  min_len  avg_len  max_len\n",
    "/home/ubuntu/20220217/peroba_align.220209_173253.England.20210701_20220228.aln  FASTA   DNA    256,146  7,659,533,838   29,903   29,903   29,903"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52a6cd13",
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove duplicate records\n",
    "nohup goalign reformat fasta -i /home/ubuntu/20220217/peroba_align.220209_173253.England.20210701_20220228.aln  --ignore-identical 0  -o /home/ubuntu/20220217/peroba_align.220209_173253.England.20210701_20220228.unique.aln"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c2e3f45",
   "metadata": {},
   "outputs": [],
   "source": [
    "seqkit stats /home/ubuntu/20220217/peroba_align.220209_173253.England.20210701_20220228.unique.aln\n",
    "\n",
    "file                                                                                   format  type  num_seqs        sum_len  min_len  avg_len  max_len\n",
    "/home/ubuntu/20220217/peroba_align.220209_173253.England.20210701_20220228.unique.aln  FASTA   DNA    256,146  7,659,533,838   29,903   29,903   29,903"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3e267e5",
   "metadata": {},
   "source": [
    "OK, so no duplicate records were found (according to `--ignore-identical 0` in `goalign reformat`, which does \"Ignore duplicated sequences that have the same name and same sequences\"), and everything is the same length as Wuhan-Wu-1--good.\n",
    "\n",
    "Next time, it might be best to remove duplicates while subsetting, as `goalign subset --ignore-identical 2`:\n",
    "    \n",
    "`--ignore-identical int   Ignore duplicated sequences that have the same name and potentially have same sequences, 0 : Does not ignore anything, 1: Ignore sequences having the same name (keep the first one whatever their sequence), 2: Ignore sequences having the same name and the same sequence`\n",
    "\n",
    "This will both save a step and avoid the problem of identical sequences being renamed.\n",
    "\n",
    "Alternatively, use `goalign dedup`, which removes duplicate sequences (considering the name, or not, depending what you tell it to do): ask Leo if `dedup` should not be used for some reason (could identical sequences with different names be worth keeping?)\n",
    "\n",
    "Also, note that next time **this would be a good place for masking problematic sites, as it hasn't been done already in Peroba**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fdb4f35",
   "metadata": {},
   "source": [
    "--------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d79ada6c",
   "metadata": {},
   "source": [
    "Make a new metadata file containing only samples from NORW:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "748e5402",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the header (column names) to a tsv\n",
    "xzcat peroba_meta.220209_173253.tsv.xz | head -1 > /home/ubuntu/20220217/peroba_meta.220209_173253.NORW.tsv\n",
    "\n",
    "#subset by 'England/NORW-' using xzgrep, and append this to the .tsv with header names you just made\n",
    "grep 'England/NORW-' peroba_meta.220209_173253.tsv.xz >> /home/ubuntu/20220217/peroba_meta.220209_173253.NORW.tsv\n",
    "\n",
    "#count how many records this gives: 3724\n",
    "xzgrep 'England/NORW-' -c peroba_meta.220209_173253.tsv.xz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fa8071b",
   "metadata": {},
   "source": [
    "From this, keep only the samples between two dates (inclusive), and save the names of those records as a separate `.txt` file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60b57f6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#subset by date: all rows between two dates\n",
    "python3\n",
    ">>> import pandas as pd\n",
    ">>> df = pd.read_csv('/home/ubuntu/20220217/peroba_meta.220209_173253.NORW.tsv', parse_dates=['date'], sep='\\t', header=0)\n",
    ">>> df['date'].min() #2021-01-31 is the earliest sample\n",
    ">>> date_subset = df.loc[df['date'].between('2021-09-01','2021-12-30', inclusive='both')]\n",
    "#this gives 1941 rows\n",
    "\n",
    "#save relevant metadata as .tsv\n",
    ">>> date_subset.to_csv('/home/ubuntu/20220217/peroba_meta.220209_173253.NORW.20210901_20211230.tsv', sep='\\t', header=True, index=False)\n",
    "#save record names (format: \"England/NORW-XXXXXXX/YYYY\" to include as a .txt\n",
    ">>> date_subset['strain'].to_csv('/home/ubuntu/20220217/peroba_meta.220209_173253.NORW.20210901_20211230.seqnames.txt', sep='\\t', header=False, index=False)\n",
    "\n",
    ">>> Ctrl+D   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef6de688",
   "metadata": {},
   "source": [
    "Get the relevant fasta records using the `.txt` file.  This is taken from a unique set (since NORW is in England and the English samples from this time were all unique, above) so no need to worry about duplicate records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a4f167d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#subset the England fasta file using the .txt file of record names: this step is a bit slow\n",
    "nohup goalign subset -f /home/ubuntu/20220217/peroba_meta.220209_173253.NORW.20210901_20211230.seqnames.txt -i /home/ubuntu/20220217/peroba_align.220209_173253.England.20210701_20220228.unique.aln -o /home/ubuntu/20220217/peroba_align.220209_173253.NORW.20210901_20211230.aln\n",
    "\n",
    "#to see if it's still running:\n",
    "ps xw\n",
    "\n",
    "#goalign subset renames identical sequence names, eg: \n",
    "#2022/02/18 11:44:45 Warning: sequence \"Japan/XXXX-XXXX-XXX/YYYY\" already exists in alignment, renamed in \"Japan/XXXX-XXXX-XXX/YYYY_0001\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aebdf52b",
   "metadata": {},
   "source": [
    "Find the 4 nearest neighbours in the England `.aln` file to each sequence.  I forgot to remove my `NORW` samples from the `England` set, so...we'll see what turns up.  The output of this program is a sequence file in `.gzip` format containing both my `NORW` sequences and their nearest neighbours."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "979b1c32",
   "metadata": {},
   "outputs": [],
   "source": [
    "#find the 4 nearest neighbours in the England .aln file to each sequence\n",
    "#template: uvaia -n 4 -r gisaid_database.aln.gz -o 4nn.aln.gz -t 12 query.aln > 4nn.txt\n",
    "nohup uvaia -n 4 -r /home/ubuntu/20220217/peroba_align.220209_173253.England.20210701_20220228.unique.aln -o /home/ubuntu/20220217/peroba_align.220209_173253.NORW.20210901_20211230.4nn.aln -t 12 /home/ubuntu/20220217/peroba_align.220209_173253.NORW.20210901_20211230.aln > /home/ubuntu/20220217/peroba_align.220209_173253.NORW.20210901_20211230.4nn.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e1703c9",
   "metadata": {},
   "source": [
    "The above line took >6h.  The file must be decompressed to work with `rapidnj`.  Next time save the output of `uvaia` to `.gz` so you don't need to use `cat`, below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e4a381a",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat /home/ubuntu/20220217/peroba_align.220209_173253.NORW.20210901_20211230.4nn.aln | gunzip > /home/ubuntu/20220217/NORW.4nn.aln"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d4d6d51",
   "metadata": {},
   "source": [
    "Check seqkit again: they're all the same length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10d49e6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "seqkit stats /home/ubuntu/20220217/peroba_align.220209_173253.NORW.20210901_20211230.aln\n",
    "    \n",
    "file                                                                         format  type  num_seqs     sum_len  min_len  avg_len  max_len\n",
    "/home/ubuntu/20220217/peroba_align.220209_173253.NORW.20210901_20211230.aln  FASTA   DNA      1,941  58,041,723   29,903   29,903   29,903"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de6aaab9",
   "metadata": {},
   "source": [
    "Remove redundant nearest neighbours.  The sequence file containing the nearest neighbours should be: `/home/ubuntu/20220217/NORW.4nn.aln`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "397c00da",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create very fast distance-based neighbour-joining tree as Newick file\n",
    "nohup rapidnj /home/ubuntu/20220217/NORW.4nn.aln -i fa -t d -n -c 8 -o t -x /home/ubuntu/20220217/NORW.4nn.aln.tre\n",
    "\n",
    "#select the 100 leaves from that tree that maximize phylogenetic diversity: these are your most interesting neighbours\n",
    "nohup iqtree -k 100 /home/ubuntu/20220217/NORW.4nn.aln.tre"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdfe879e",
   "metadata": {},
   "source": [
    "`/home/ubuntu/20220217/NORW.4nn.aln.tre.pda` now includes the list of 100 non-redundant neighbour sequences and the pruned 4nn tree.\n",
    "\n",
    "Take the list of nns to use and save it as `seqs_to_keep.txt`, being careful to reformat the names to match the alignment file.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "898a28fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "grep -A 100 \"The optimal PD set has 100 taxa:\" /home/ubuntu/20220217/NORW.4nn.aln.tre.pda | sed '1d'  | sed 's/^.//;s/.$//;s/_/\\//;s/_/\\//' > /home/ubuntu/20220217/seqs_to_keep.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a77b7d65",
   "metadata": {},
   "source": [
    "Now use this list to subset the 4nn.aln file containing all potential neighbour sequences.  When I tried to subset from the NORW file, the output file was empty, so `uvaia` must have taken care that the neighbours are not the sequences themselves.  However, note that many of the reduced refs are `NORW` sequences themselves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "922b88b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "nohup goalign subset -f /home/ubuntu/20220217/seqs_to_keep.txt -i /home/ubuntu/20220217/peroba_align.220209_173253.England.20210701_20220228.unique.aln -o /home/ubuntu/20220217/reduced_refs.aln"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60fddfbb",
   "metadata": {},
   "source": [
    "Next, prune the tree that you made from all the potential neighbours with `rapidnj` down to just the seqs to keep:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a2b38da",
   "metadata": {},
   "outputs": [],
   "source": [
    "nohup gotree prune -r --tipfile /home/ubuntu/20220217/seqs_to_keep.txt -i /home/ubuntu/20220217/NORW.4nn.aln.tre -o /home/ubuntu/20220217/reduced_refs2.tre"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2625fc72",
   "metadata": {},
   "source": [
    "This didn't work (`The tree after tip removal is only made of two tips`), so I'm going to make a list called `leftovers.txt` of sequences to remove instead and use that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04c36f0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "grep \">\" /home/ubuntu/20220217/NORW.4nn.aln | sed 's/>//' > /home/ubuntu/20220217/allseqs.txt\n",
    "grep -wv -f /home/ubuntu/20220217/seqs_to_keep.txt /home/ubuntu/20220217/allseqs.txt > /home/ubuntu/20220217/leftovers.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c7946b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "nohup gotree prune --tipfile /home/ubuntu/20220217/leftovers.txt -i /home/ubuntu/20220217/NORW.4nn.aln.tre -o /home/ubuntu/20220217/reduced_refs.tre"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fa2356e",
   "metadata": {},
   "source": [
    "This tree has 3991 tips!  This was not supposed to happen..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20701823",
   "metadata": {},
   "source": [
    "Finally, concatenate the neighbour and query sequence files together, and build a new maximum likelihood tree from all those sequences, using the non-redundant neighbours as the backbone for the tree (this is what `-g` does in `iqtree`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c82410c",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat /home/ubuntu/20220217/reduced_refs.aln /home/ubuntu/20220217/peroba_align.220209_173253.NORW.20210901_20211230.aln > /home/ubuntu/20220217/allseqs.aln\n",
    "iqtree -s /home/ubuntu/20220217/allseqs.aln -m HKY+G -g /home/ubuntu/20220217/reduced_refs.tre -t PARS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "421cd1e3",
   "metadata": {},
   "source": [
    "`iqtree` throws an error saying that there are duplicated NORW sequences in the alignment, and to rename some of them.  So, maybe some of the neighbours were the tips themselves after all.  The internet is cutting out so much here so I'm going home.  Will get back to it tonight!\n",
    "\n",
    "A quick fix for now is `goalign dedup /home/ubuntu/20220217/allseqs.aln -o /home/ubuntu/20220217/allseqs_dedup.aln`\n",
    "That failed, so `seqkit rmdup -s < /home/ubuntu/20220217/allseqs.aln > /home/ubuntu/20220217/allseqs_dedup.aln`: this worked.\n",
    "\n",
    "Try again: `iqtree -s /home/ubuntu/20220217/allseqs_dedup.aln -m HKY+G -g /home/ubuntu/20220217/reduced_refs.tre -t PARS`\n",
    "\n",
    "This caused many errors of the form:\n",
    "\n",
    "`ERROR: ERROR: Taxon <seqname> in constraint tree does not appear in full tree`, for 3991\n",
    " taxa.\n",
    "\n",
    "So, something's wrong with `reduced_refs.tre`.  I suspect the tree doesn't have the same samples in it as `reduced_refs.aln`, which is supported by `gotree stats` crashing.  What would happen if, instead of pruning an existing tree down to the reduced refs, I took the reduced refs and built a tree from scratch with them?  Why was pruning suggested over rebuilding?  Just for speed, or is there a bigger reason?\n",
    "\n",
    "#I'm going to try this again, but first I'm going to cut all the overlapping sequences between the NORW set out from #the potential nearest neighbours set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e68065c",
   "metadata": {},
   "source": [
    "remake the reduced_refs tree using iqtree to begin with\n",
    "\n",
    "    iqtree -s reduced_refs.aln\n",
    "\n",
    "The Newick tree is in `reduced_refs.aln.treefile`.  gotree stats struggles even with this, so...maybe:\n",
    "\n",
    "    grep -o -i \"England\" reduced_refs.aln.treefile | wc -l\n",
    "    \n",
    "This will count the number of sample names with \"England\" in them in the Newick file.  In here are 100!  So that should work.\n",
    "\n",
    "Gingerly try adding the NORW sequences in:\n",
    "\n",
    "    iqtree -s /home/ubuntu/20220217/allseqs.aln -m HKY+G -g /home/ubuntu/20220217/reduced_refs.aln.treefile -t PARS\n",
    "\n",
    "This finds 48 duplicated sequences.  Check with `grep` to see if you find the same:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad876ae4",
   "metadata": {},
   "source": [
    "    grep \">\" reduced_refs.aln > reduced_refs_names.txt\n",
    "    grep \">\" /home/ubuntu/20220217/peroba_align.220209_173253.NORW.20210901_20211230.aln > NORW_names.txt\n",
    "    grep -f reduced_refs_names.txt NORW_names.txt | grep -c \"England\"\n",
    "\n",
    "yes, 48 sequences are duplicated between the refs and the references.\n",
    "\n",
    "so, go through the neighbours from uvaia and take out the sequences that are also found in NORW.\n",
    "Potential nearest neighbours multiline fasta: `/home/ubuntu/20220217/NORW.4nn.aln`\n",
    "NORW multiline fasta: `/home/ubuntu/20220217/peroba_align.220209_173253.NORW.20210901_20211230.aln`\n",
    "\n",
    "    grep \">\" /home/ubuntu/20220217/NORW.4nn.aln > /home/ubuntu/20220217/4nn_names.txt\n",
    "    grep -vf NORW_names.txt 4nn_names.txt > unique_neighbours.txt \n",
    "    \n",
    "nope, that took out 1729, I just want 48 gone\n",
    "\n",
    "For the record:\n",
    "    \n",
    "    grep -c \"England\" /home/ubuntu/20220217/reduced_refs_names.txt \n",
    "    100\n",
    "    grep -c \"England\" /home/ubuntu/20220217/NORW_names.txt \n",
    "    1941\n",
    "    grep -c \"England\" 4nn_names.txt \n",
    "    3991\n",
    "\n",
    "    grep -vw -f NORW_names.txt 4nn_names.txt | sed 's/>//' > unique_neighbours_2.txt \n",
    "This still gives me 2262.  So 1729 NORW names made it into the nearest neighbours pool: remembering that they haven't been reduced yet, this is actually fine.\n",
    "\n",
    "Subset the fasta file of nearest neighbours:\n",
    "\n",
    "    nohup goalign subset -f /home/ubuntu/20220217/unique_neighbours_2.txt -i /home/ubuntu/20220217/NORW.4nn.aln -o /home/ubuntu/20220217/NORW.4nn.no_overlap.aln\n",
    "    \n",
    "Note that it's important for goalign subset that there are no \">\" in the sequence names .txt, otherwise it won't run.\n",
    "\n",
    "Now reduce that dataset:\n",
    "\n",
    "#create very fast distance-based neighbour-joining tree as Newick file\n",
    "\n",
    "    nohup rapidnj /home/ubuntu/20220217/NORW.4nn.no_overlap.aln -i fa -t d -n -c 8 -o t -x /home/ubuntu/20220217/NORW.4nn.no_overlap.aln.tre\n",
    "    \n",
    "#count how many leaves there are now:\n",
    "\n",
    "    grep -o -i \"England\" /home/ubuntu/20220217/NORW.4nn.no_overlap.aln.tre | wc -l\n",
    "    \n",
    "2262!  great.\n",
    "\n",
    "#select the 100 leaves from that tree that maximize phylogenetic diversity: these are your most interesting neighbours\n",
    "\n",
    "    nohup iqtree -k 100 /home/ubuntu/20220217/NORW.4nn.no_overlap.aln.tre\n",
    "    \n",
    "#get their names:\n",
    "    \n",
    "    grep -A 100 \"The optimal PD set has 100 taxa:\" /home/ubuntu/20220217/NORW.4nn.no_overlap.aln.tre.pda | sed '1d'  | sed 's/^.//;s/.$//;s/_/\\//;s/_/\\//' > /home/ubuntu/20220217/seqs_to_keep_no_overlap.txt\n",
    "    \n",
    "#subset the neighbours fasta file again:\n",
    "\n",
    "    nohup goalign subset -f /home/ubuntu/20220217/seqs_to_keep_no_overlap.txt -i /home/ubuntu/20220217/NORW.4nn.no_overlap.aln -o /home/ubuntu/20220217/reduced_refs_no_overlap.aln\n",
    "    \n",
    "    \n",
    "#make a tree out of those:\n",
    "    \n",
    "    iqtree -s /home/ubuntu/20220217/reduced_refs_no_overlap.aln\n",
    "     \n",
    "    \n",
    "#count again to be sure:\n",
    "\n",
    "    grep -o -i \"England\" reduced_refs_no_overlap.aln.treefile | wc -l\n",
    "    \n",
    "#it's 100 as hoped\n",
    "    \n",
    "#concatenate NORW and reduced ref sequences together:\n",
    "    cat /home/ubuntu/20220217/reduced_refs_no_overlap.aln /home/ubuntu/20220217/peroba_align.220209_173253.NORW.20210901_20211230.aln > /home/ubuntu/20220217/allseqs_no_overlap.aln\n",
    "\n",
    "#and now try adding the NORW sequences to that backbone.  Note that allseqs is important because there is no sequence data in the .tre itself at the moment.\n",
    "\n",
    "    iqtree -s /home/ubuntu/20220217/allseqs_no_overlap.aln -m HKY+G -g /home/ubuntu/20220217/reduced_refs_no_overlap.aln.treefile -t PARS\n",
    "    \n",
    "Also note that this is...kind of a ridiculous tree.  You've got ___ NORW query sequences and only 100 reference sequences, which seems unfair.  Oh dear, it's taking forever to run...\n",
    "\n",
    "Finally, it is in figtree /home/ubuntu/20220217/allseqs_no_overlap.aln.treefile.\n",
    "\n",
    "A fun thing to do with this, now that it's done, is to try treetime mugration again, with the more interesting regions now.  Hey, this is what you set out to do in the bullet points at the top anyway.\n",
    "\n",
    "Quick quick, treetime:\n",
    "\n",
    "#get list of leaf tips\n",
    "grep \"England\" /home/ubuntu/20220217/allseqs_no_overlap.aln | sed 's/>//g'  > /home/ubuntu/20220217/leaftips.txt\n",
    "\n",
    "#The file \"'/home/ubuntu/20220217/peroba_meta.220209_173253.England.20210701_20220228.tsv'\" has been squashed a bit and needs some reformatting.\n",
    "\n",
    "     1) location_cogukage --> location_coguk    age\n",
    "     2) Europe / United Kingdom / England --> Europe/UnitedKingdom/England\n",
    "     3) make sure header is tab-separated\n",
    "     \n",
    "     sed -i 's/ \\/ /\\//g;s/location_cogukage/location_coguk    age/;s/United\\s\\+Kingdom/UnitedKingdom/;s/\\s\\+/\\t/g' /home/ubuntu/20220217/peroba_meta.220209_173253.England.20210701_20220228.tsv\n",
    "     \n",
    "python3\n",
    "readfile = '/home/ubuntu/20220217/peroba_meta.220209_173253.England.20210701_20220228.tsv'\n",
    "df = pd.read_csv(readfile, header=0, sep='\\t', dtype='str')\n",
    "df = df.rename(columns={'strain':'name'}) #header required by treetime\n",
    "\n",
    "#include only the names that match the names in the alignment\n",
    "leaf_df = pd. read_csv('/home/ubuntu/20220217/leaftips.txt', header=None)\n",
    "leaf_df = leaf_df.rename(columns={0:'name'})\n",
    "\n",
    "#add dates to leaf_df, and format the names like in the tree\n",
    "save_df = leaf_df.merge(df, how='left')\n",
    "save_df['name'] = save_df['name'].str.replace('/', '_')\n",
    "print(save_df)\n",
    "\n",
    "save_df.to_csv('/home/ubuntu/20220217/treetime_dates.tsv', sep='\\t', index=False)\n",
    "\n",
    "Once you've done that, make this into .nf format so it will be easier to run.  Or a bash script.  Either one.  Nextflow is nice because it should deal with intermediate files for you and because it can run things in parallel/specify threads.\n",
    "\n",
    "\n",
    "Try this again:\n",
    " \n",
    "     sed 's/ \\/ /\\//g;s/location_cogukage/location_coguk    age/;1s/\\s\\+/\\t/g' /home/ubuntu/20220217/peroba_meta.220209_173253.England.20210701_20220228.tsv > fritest2.tsv\n",
    "     \n",
    "This works!\n",
    "\n",
    "Now remake treetime dates quickly....done.\n",
    "\n",
    "now the leaf tips and sequence names don't add up, so quickly change '/' to '_' in the alignment file:\n",
    "sed -i 's/\\//_/g' /home/ubuntu/20220217/allseqs_no_overlap.aln\n",
    "\n",
    "Treetime on default:\n",
    "\n",
    "! treetime --tree /home/ubuntu/20220217/allseqs_no_overlap.aln.treefile --dates /home/ubuntu/20220217/treetime_dates.tsv --aln /home/ubuntu/20220217/allseqs_no_overlap.aln --outdir /home/ubuntu/20220217/timetree\n",
    "\n",
    "Treetime with mugration:\n",
    "\n",
    "add this bit to the code that gets treetime dates:\n",
    "df['country'] = df['strain'].str.split('/').str[0]\n",
    "df['region'] = df['strain'].str.split('/').str[1]\n",
    "df['region'] = df['region'].str.split('-').str[0]\n",
    "\n",
    "run this: \n",
    "python /home/ubuntu/scripts/get_treetime_dates.py --metadata_tsv fritest2.tsv --leaftips /home/ubuntu/20220217/leaftips.txt --out /home/ubuntu/20220217/treetime_dates_2.tsv\n",
    "\n",
    "now run a mugration model:\n",
    "! treetime mugration --tree /home/ubuntu/20220217/allseqs_no_overlap.aln.treefile --states /home/ubuntu/20220217/treetime_dates_2.tsv --attribute 'region' --outdir /home/ubuntu/20220217/timetree-mugration\n",
    "\n",
    "The output tree is not annotated, I suspect because so many branch lengths are 0.  How could it infer the region if everything is 0?  To fix this, I will try again with more diverse sequences.  Maybe the 100 most diverse NORW, and the 100 most diverse neighbours.  For now, though, go eat something (done!) and then write up the streamlining.  Then retry.  Then, once you have a tree, visualize it.\n",
    "\n",
    "--rapidnj of NORW, iqtree -k 100\n",
    "--rapidnj of nn possibilities, k -100\n",
    "--concatenate, reanalyse\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f222af01",
   "metadata": {},
   "source": [
    "**Appendix of trying out gotree prune yet again and still failing**\n",
    "\n",
    "Quickly just check if I had goalign prune backwards:\n",
    "\n",
    "    nohup gotree prune -r --tipfile /home/ubuntu/20220217/NORW.4nn.no_overlap.aln.tre -i /home/ubuntu/20220217/seqs_to_keep_no_overlap.txt -o /home/ubuntu/20220217/reduced_refs_no_overlap.tre\n",
    "    \n",
    "Nope, that does nothing.  Try it forwards again?\n",
    "\n",
    "    nohup gotree prune -r --tipfile /home/ubuntu/20220217/seqs_to_keep_no_overlap.txt  -i /home/ubuntu/20220217/NORW.4nn.no_overlap.aln.tre -o /home/ubuntu/20220217/reduced_refs_no_overlap.tre\n",
    "    \n",
    "This says that \"The tree after tip removal is only made of two tips\" again.  Hmm...this isn't true because I'm asking it to keep 100 tips.  The names are formatted the same in the .tre and the .txt.\n",
    "    \n",
    "If I drop the --reverse, I get a real tree:\n",
    "    \n",
    "    gotree prune --tipfile /home/ubuntu/20220217/seqs_to_keep_no_overlap.txt  -i /home/ubuntu/20220217/NORW.4nn.no_overlap.aln.tre -o /home/ubuntu/20220217/reduced_refs_no_overlap.tre\n",
    "\n",
    "Last thing to try is dropping the --reverse and changing `seqs_to_keep` to `seqs_to_prune` for the input:\n",
    "    \n",
    "    #get seqs_to_prune\n",
    "    \n",
    "    grep -vw -f /home/ubuntu/20220217/seqs_to_keep_no_overlap.txt /home/ubuntu/20220217/unique_neighbours_2.txt | sed 's/>//' > /home/ubuntu/20220217/seqs_to_prune.txt   #2162 records\n",
    "    \n",
    "    gotree prune --tipfile /home/ubuntu/20220217/seqs_to_prune.txt  -i /home/ubuntu/20220217/NORW.4nn.no_overlap.aln.tre -o /home/ubuntu/20220217/reduced_refs_no_overlap.tre #2262 records at the end, same number as if it hadn't been pruned at all....so weird\n",
    "    \n",
    "Okay, so I'm leaving goalign prune and going with iqtree."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4109e8a9",
   "metadata": {},
   "source": [
    "**Congratulations!  Preprocessing and all that is done.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2835324",
   "metadata": {},
   "source": [
    "P.S.  To linearize a fasta file:\n",
    "    \n",
    "`awk '/^>/ {printf(\"\\n%s\\n\",$0);next; } { printf(\"%s\",$0);}  END {printf(\"\\n\");}' < /home/ubuntu/20220217/peroba_align.220209_173253.England.20210701_20220228.unique.aln | sed '1d' > /home/ubuntu/20220217/peroba_align.220209_173253.England.20210701_20220228.unique.1.aln`\n",
    "\n",
    "\n",
    "`awk '/^>/ {printf(\"\\n%s\\n\",$0);next; } { printf(\"%s\",$0);}  END {printf(\"\\n\");}' < /home/ubuntu/20220217/peroba_align.220209_173253.NORW.20210901_20211230.aln | sed '1d' > /home/ubuntu/20220217/peroba_align.220209_173253.NORW.20210901_20211230.1.aln`\n",
    "\n",
    "https://www.biostars.org/p/9262/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87e2d2bb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
